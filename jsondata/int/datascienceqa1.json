{"data":[{"id":1,"text":"1. What are the differences between supervised and unsupervised learning?\nSupervised Learning\n\nUses known and labeled data as input\nSupervised learning has a feedback mechanism \nThe most commonly used supervised learning algorithms are decision trees, logistic regression, and support vector machine\n\nUnsupervised Learning\nUses unlabeled data as input\nUnsupervised learning has no feedback mechanism \nThe most commonly used unsupervised learning algorithms are k-means clustering, hierarchical clustering, and apriori algorithm"},{"id":2,"text":"2. How is logistic regression done?\nLogistic regression measures the relationship between the dependent variable (our label of what we want to predict) and one or more independent variables (our features) by estimating probability using its underlying logistic function (sigmoid).\n\nThe image shown below depicts how logistic regression works:\n\n \n\nThe formula and graph for the sigmoid function are as shown:"},{"id":3,"text":"3. Explain the steps in making a decision tree.\nTake the entire data set as input\nCalculate entropy of the target variable, as well as the predictor attributes\nCalculate your information gain of all attributes (we gain information on sorting different objects from each other)\nChoose the attribute with the highest information gain as the root node \nRepeat the same procedure on every branch until the decision node of each branch is finalized\nFor example, let's say you want to build a decision tree to decide whether you should accept or decline a job offer. The decision tree for this case is as shown:\n\nIt is clear from the decision tree that an offer is accepted if:\n\nSalary is greater than $50,000\nThe commute is less than an hour \nIncentives are offered"},{"id":4,"text":"4. How do you build a random forest model?\n\nA random forest is built up of a number of decision trees. If you split the data into different packages and make a decision tree in each of the different groups of data, the random forest brings all those trees together.\n\nSteps to build a random forest model:\nRandomly select 'k' features from a total of 'm' features where k << m\nAmong the 'k' features, calculate the node D using the best split point\nSplit the node into daughter nodes using the best split\nRepeat steps two and three until leaf nodes are finalized \nBuild forest by repeating steps one to four for 'n' times to create 'n' number of trees"},{"id":5,"text":"5. How can you avoid overfitting your model?\nOverfitting refers to a model that is only set for a very small amount of data and ignores the bigger picture. There are three main methods to avoid overfitting:\n\nKeep the model simple—take fewer variables into account, thereby removing some of the noise in the training data\nUse cross-validation techniques, such as k folds cross-validation \nUse regularization techniques, such as LASSO, that penalize certain model parameters if they're likely to cause overfitting"},{"id":6,"text":"6. Differentiate between univariate, bivariate, and multivariate analysis.\nUnivariate\nUnivariate data contains only one variable. The purpose of the univariate analysis is to describe the data and find patterns that exist within it. \n\nExample: height of students \n\nHeight (in cm)\n\n164\n\n167\n\n170\n\n174\n\n178\n\n180\n\nThe patterns can be studied by drawing conclusions using mean, median, mode, dispersion or range, minimum, maximum, etc.\n\nBivariate\nBivariate data involves two different variables. The analysis of this type of data deals with causes and relationships and the analysis is done to determine the relationship between the two variables.\n\nExample: temperature and ice cream sales in the summer season\n\nTemperature (in Celcius)\n\nSales\n\n20\n\n2,000\n\n25\n\n2,100\n\n26\n\n2,300\n\n28\n\n2,400\n\n30\n\n2,600\n\n36\n\n3,100\n\nHere, the relationship is visible from the table that temperature and sales are directly proportional to each other. The hotter the temperature, the better the sales.\n\nMultivariate\nMultivariate data involves three or more variables, it is categorized under multivariate. It is similar to a bivariate but contains more than one dependent variable.\n\nExample: data for house price prediction \n\nNo. of rooms\n\nFloors\n\nArea (sq ft)\n\nPrice\n\n2\n\n0\n\n900\n\n$4000,00\n\n3\n\n2\n\n1,100\n\n$600,000\n\n3\n\n5\n\n1,500\n\n$900,000\n\n4\n\n3\n\n2,100\n\n$1,200,000\n\nThe patterns can be studied by drawing conclusions using mean, median, and mode, dispersion or range, minimum, maximum, etc. You can start describing the data and using it to guess what the price of the house will be."},{"id":7,"text":"7. What are the feature selection methods used to select the right variables?\n\nThere are two main methods for feature selection, i.e, filter, and wrapper methods.\n\nFilter Methods\nThis involves: \n\nLinear discrimination analysis\nANOVA\nChi-Square\nThe best analogy for selecting features is \"bad data in, bad answer out.\" When we're limiting or selecting the features, it's all about cleaning up the data coming in. \n\nWrapper Methods\nThis involves: \n\nForward Selection: We test one feature at a time and keep adding them until we get a good fit\nBackward Selection: We test all the features and start removing them to see what works better\nRecursive Feature Elimination: Recursively looks through all the different features and how they pair together\nWrapper methods are very labor-intensive, and high-end computers are needed if a lot of data analysis is performed with the wrapper method."},{"id":8,"text":"8. In your choice of language, write a program that prints the numbers ranging from one to 50.\nBut for multiples of three, print \"Fizz\" instead of the number, and for the multiples of five, print \"Buzz.\" For numbers which are multiples of both three and five, print \"FizzBuzz\" \n\n \n\nNote that the range mentioned is 51, which means zero to 50. However, the range asked in the question is one to 50. Therefore, in the above code, you can include the range as (1,51).\n\nThe output of the above code is as shown:"},{"id":9,"text":"9. You are given a data set consisting of variables with more than 30 percent missing values. How will you deal with them?\nThe following are ways to handle missing data values:\n\nIf the data set is large, we can just simply remove the rows with missing data values. It is the quickest way; we use the rest of the data to predict the values.\n\nFor smaller data sets, we can substitute missing values with the mean or average of the rest of the data using the pandas' data frame in python. There are different ways to do so, such as df.mean(), df.fillna(mean).\n\n \n\nOffer Expires In\n\n00 : HRS\n59 : MIN\n27SEC"},{"id":10,"text":"10. For the given points, how will you calculate the Euclidean distance in Python?\nplot1 = [1,3]\n\nplot2 = [2,5]\n\nThe Euclidean distance can be calculated as follows:\n\neuclidean_distance = sqrt( (plot1[0]-plot2[0])**2 + (plot1[1]-plot2[1])**2 )\n\nCheck out the Simplilearn's video on \"Data Science Interview Question\" curated by industry experts to help you prepare for an interview."},{"id":11,"text":"11. What are dimensionality reduction and its benefits?\nThe Dimensionality reduction refers to the process of converting a data set with vast dimensions into data with fewer dimensions (fields) to convey similar information concisely. \n\nThis reduction helps in compressing data and reducing storage space. It also reduces computation time as fewer dimensions lead to less computing. It removes redundant features; for example, there's no point in storing a value in two different units (meters and inches)."},{"id":12,"text":"12. How will you calculate eigenvalues and eigenvectors of the following 3x3 matrix?\n-2\n\n-4\n\n2\n\n-2\n\n1\n\n2\n\n4\n\n2\n\n5\n\nThe characteristic equation is as shown:\n\nExpanding determinant:\n\n(-2 – λ) [(1-λ) (5-λ)-2x2] + 4[(-2) x (5-λ) -4x2] + 2[(-2) x 2-4(1-λ)] =0\n\n- λ3 + 4λ2 + 27λ – 90 = 0,\n\nλ3 - 4 λ2 -27 λ + 90 = 0\n\nHere we have an algebraic equation built from the eigenvectors.\n\nBy hit and trial:\n\n33 – 4 x 32 - 27 x 3 +90 = 0\n\nHence, (λ - 3) is a factor:\n\nλ3 - 4 λ2 - 27 λ +90 = (λ – 3) (λ2 – λ – 30)\n\nEigenvalues are 3,-5,6:\n\n(λ – 3) (λ2 – λ – 30) = (λ – 3) (λ+5) (λ-6),\n\nCalculate eigenvector for λ = 3\n\nFor X = 1,\n\n-5 - 4Y + 2Z =0,\n\n-2 - 2Y + 2Z =0\n\nSubtracting the two equations: \n\n3 + 2Y = 0,\n\nSubtracting back into second equation:\n\nY = -(3/2) \n\nZ = -(1/2)\n\nSimilarly, we can calculate the eigenvectors for -5 and 6."},{"id":13,"text":"13. How should you maintain a deployed model?\nThe steps to maintain a deployed model are:\n\nMonitor \nConstant monitoring of all models is needed to determine their performance accuracy. When you change something, you want to figure out how your changes are going to affect things. This needs to be monitored to ensure it's doing what it's supposed to do.\n\nEvaluate\nEvaluation metrics of the current model are calculated to determine if a new algorithm is needed. \n\nCompare\nThe new models are compared to each other to determine which model performs the best. \n\nRebuild\nThe best performing model is re-built on the current state of data."},{"id":14,"text":"14. What are recommender systems?\nA recommender system predicts what a user would rate a specific product based on their preferences. It can be split into two different areas:\n\nCollaborative Filtering\nAs an example, Last.fm recommends tracks that other users with similar interests play often. This is also commonly seen on Amazon after making a purchase; customers may notice the following message accompanied by product recommendations: \"Users who bought this also bought…\"\n\nContent-based Filtering\nAs an example: Pandora uses the properties of a song to recommend music with similar properties. Here, we look at content, instead of looking at who else is listening to music."},{"id":15,"text":"15. How do you find RMSE and MSE in a linear regression model?\n\nRMSE and MSE are two of the most common measures of accuracy for a linear regression model. \n\nRMSE indicates the Root Mean Square Error. \n\nMSE indicates the Mean Square Error."},{"id":16,"text":"16. How can you select k for k-means? \nWe use the elbow method to select k for k-means clustering. The idea of the elbow method is to run k-means clustering on the data set where 'k' is the number of clusters.\n\nWithin the sum of squares (WSS), it is defined as the sum of the squared distance between each member of the cluster and its centroid."},{"id":17,"text":"17. What is the significance of p-value?\np-value typically ≤ 0.05\n\nThis indicates strong evidence against the null hypothesis; so you reject the null hypothesis.\n\np-value typically > 0.05\n\nThis indicates weak evidence against the null hypothesis, so you accept the null hypothesis. \n\np-value at cutoff 0.05 \n\nThis is considered to be marginal, meaning it could go either way."},{"id":18,"text":"18. How can outlier values be treated?\nYou can drop outliers only if it is a garbage value. \n\nExample: height of an adult = abc ft. This cannot be true, as the height cannot be a string value. In this case, outliers can be removed.\n\nIf the outliers have extreme values, they can be removed. For example, if all the data points are clustered between zero to 10, but one point lies at 100, then we can remove this point.\n\nIf you cannot drop outliers, you can try the following:\n\nTry a different model. Data detected as outliers by linear models can be fit by nonlinear models. Therefore, be sure you are choosing the correct model.\nTry normalizing the data. This way, the extreme data points are pulled to a similar range.\nYou can use algorithms that are less affected by outliers; an example would be random forests."},{"id":19,"text":"19. How can time-series data be declared as stationery?\n\nIt is stationary when the variance and mean of the series are constant with time. \n\nHere is a visual example: \n\n \n\nIn the first graph, the variance is constant with time. Here, X is the time factor and Y is the variable. The value of Y goes through the same points all the time; in other words, it is stationary.\n\nIn the second graph, the waves get bigger, which means it is non-stationary and the variance is changing with time."},{"id":20,"text":"20. How can you calculate accuracy using a confusion matrix?\nConsider this confusion matrix:\n\nYou can see the values for total data, actual values, and predicted values.\n\nThe formula for accuracy is:\n\nAccuracy = (True Positive + True Negative) / Total Observations\n\n= (262 + 347) / 650\n\n= 609 / 650\n\n= 0.93\n\nAs a result, we get an accuracy of 93 percent."},{"id":21,"text":"21. Write the equation and calculate the precision and recall rate.\nConsider the same confusion matrix used in the previous question.\n\nPrecision = (True positive) / (True Positive + False Positive)\n\n= 262 / 277\n\n= 0.94\n\nRecall Rate = (True Positive) / (Total Positive + False Negative)\n\n= 262 / 288\n\n= 0.90"},{"id":22,"text":"22. 'People who bought this also bought…' recommendations seen on Amazon are a result of which algorithm?\nThe recommendation engine is accomplished with collaborative filtering. Collaborative filtering explains the behavior of other users and their purchase history in terms of ratings, selection, etc. \n\nThe engine makes predictions on what might interest a person based on the preferences of other users. In this algorithm, item features are unknown.\n\n \n\nFor example, a sales page shows that a certain number of people buy a new phone and also buy tempered glass at the same time. Next time, when a person buys a phone, he or she may see a recommendation to buy tempered glass as well."},{"id":23,"text":"23. Write a basic SQL query that lists all orders with customer information.\nUsually, we have order tables and customer tables that contain the following columns:\n\nOrder Table \nOrderid\ncustomerId \nOrderNumber\nTotalAmount\nCustomer Table \nId\nFirstName\nLastName\nCity \nCountry  \nThe SQL query is:\nSELECT OrderNumber, TotalAmount, FirstName, LastName, City, Country\nFROM Order\nJOIN Customer\nON Order.CustomerId = Customer.Id"},{"id":24,"text":"24. You are given a dataset on cancer detection. You have built a classification model and achieved an accuracy of 96 percent. Why shouldn't you be happy with your model performance? What can you do about it?\nCancer detection results in imbalanced data. In an imbalanced dataset, accuracy should not be based as a measure of performance. It is important to focus on the remaining four percent, which represents the patients who were wrongly diagnosed. Early diagnosis is crucial when it comes to cancer detection, and can greatly improve a patient's prognosis.\n\nHence, to evaluate model performance, we should use Sensitivity (True Positive Rate), Specificity (True Negative Rate), F measure to determine the class wise performance of the classifier."},{"id":25,"text":"25. Which of the following machine learning algorithms can be used for inputting missing values of both categorical and continuous variables?\nK-means clustering\nLinear regression \nK-NN (k-nearest neighbor)\nDecision trees \nThe K nearest neighbor algorithm can be used because it can compute the nearest neighbor and if it doesn't have a value, it just computes the nearest neighbor based on all the other features. \n\nWhen you're dealing with K-means clustering or linear regression, you need to do that in your pre-processing, otherwise, they'll crash. Decision trees also have the same problem, although there is some variance."},{"id":26,"text":"26. Below are the eight actual values of the target variable in the train file. What is the entropy of the target variable?\n\n[0, 0, 0, 1, 1, 1, 1, 1] \n\nChoose the correct answer.\n\n-(5/8 log(5/8) + 3/8 log(3/8))\n5/8 log(5/8) + 3/8 log(3/8)\n3/8 log(5/8) + 5/8 log(3/8)\n5/8 log(3/8) – 3/8 log(5/8)\nThe target variable, in this case, is 1. \n\nThe formula for calculating the entropy is:\n\nPutting p=5 and n=8, we get \n\nEntropy = A = -(5/8 log(5/8) + 3/8 log(3/8))"},{"id":27,"text":"27. We want to predict the probability of death from heart disease based on three risk factors: age, gender, and blood cholesterol level. What is the most appropriate algorithm for this case?\nChoose the correct option:\n\nLogistic Regression \nLinear Regression\nK-means clustering \nApriori algorithm\nThe most appropriate algorithm for this case is A, logistic regression."},{"id":28,"text":"28. After studying the behavior of a population, you have identified four specific individual types that are valuable to your study. You would like to find all users who are most similar to each individual type. Which algorithm is most appropriate for this study?\nChoose the correct option:\n\nK-means clustering\nLinear regression\nAssociation rules\nDecision trees\nAs we are looking for grouping people together specifically by four different similarities, it indicates the value of k. Therefore, K-means clustering (answer A) is the most appropriate algorithm for this study."},{"id":29,"text":"29. You have run the association rules algorithm on your dataset, and the two rules {banana, apple} => {grape} and {apple, orange} => {grape} have been found to be relevant. What else must be true?\nChoose the right answer:\n\n{banana, apple, grape, orange} must be a frequent itemset\n{banana, apple} => {orange} must be a relevant rule\n{grape} => {banana, apple} must be a relevant rule\n{grape, apple} must be a frequent itemset\nThe answer is A: {grape, apple} must be a frequent itemset"},{"id":30,"text":"30. Your organization has a website where visitors randomly receive one of two coupons. It is also possible that visitors to the website will not receive a coupon. You have been asked to determine if offering a coupon to website visitors has any impact on their purchase decisions. Which analysis method should you use?\nOne-way ANOVA \nK-means clustering\nAssociation rules \nStudent's t-test \nThe answer is A: One-way ANOVA"},{"id":31,"text":"31. What do you understand about true positive rate and false-positive rate?\nThe True Positive Rate (TPR) defines the probability that an actual positive will turn out to be positive. \nThe True Positive Rate (TPR) is calculated by taking the ratio of the [True Positives (TP)] and [True Positive (TP) & False Negatives (FN) ]. \n\nThe formula for the same is stated below -\n\nTPR=TP/TP+FN\n\nThe False Positive Rate (FPR) defines the probability that an actual negative result will be shown as a positive one i.e the probability that a model will generate a false alarm. \nThe False Positive Rate (FPR) is calculated by taking the ratio of the [False Positives (FP)] and [True Positives (TP) & False Positives(FP)].\n\nThe formula for the same is stated below -\n\nFPR=FP/TP+FP"},{"id":32,"text":"32. What is the ROC curve?\nThe graph between the True Positive Rate on the y-axis and the False Positive Rate on the x-axis is called the ROC curve and is used in binary classification.\n\nThe False Positive Rate (FPR) is calculated by taking the ratio between False Positives and the total number of negative samples, and the True Positive Rate (TPR) is calculated by taking the ratio between True Positives and the total number of positive samples.\n\nIn order to construct the ROC curve, the TPR and FPR values are plotted on multiple threshold values. The area range under the ROC curve has a range between 0 and 1. A completely random model, which is represented by a straight line, has a 0.5 ROC. The amount of deviation a ROC has from this straight line denotes the efficiency of the model.\n\n \n\nThe image above denotes a ROC curve example.\n\nBasic Data Science Interview Questions\nLet us begin with a few basic data science interview questions!"},{"id":33,"text":"33. What are the feature vectors?\nA feature vector is an n-dimensional vector of numerical features that represent an object. In machine learning, feature vectors are used to represent numeric or symbolic characteristics (called features) of an object in a mathematical way that's easy to analyze."},{"id":34,"text":"34. What are the steps in making a decision tree?\nTake the entire data set as input.\nLook for a split that maximizes the separation of the classes. A split is any test that divides the data into two sets.\nApply the split to the input data (divide step).\nRe-apply steps one and two to the divided data.\nStop when you meet any stopping criteria.\nThis step is called pruning. Clean up the tree if you went too far doing splits."},{"id":35,"text":"35. What is root cause analysis?\nRoot cause analysis was initially developed to analyze industrial accidents but is now widely used in other areas. It is a problem-solving technique used for isolating the root causes of faults or problems. A factor is called a root cause if its deduction from the problem-fault-sequence averts the final undesirable event from recurring."},{"id":36,"text":"36. What is logistic regression?\nLogistic regression is also known as the logit model. It is a technique used to forecast the binary outcome from a linear combination of predictor variables."},{"id":37,"text":"37. What are recommender systems?\nRecommender systems are a subclass of information filtering systems that are meant to predict the preferences or ratings that a user would give to a product."},{"id":38,"text":"38. Explain cross-validation.\nCross-validation is a model validation technique for evaluating how the outcomes of a statistical analysis will generalize to an independent data set. It is mainly used in backgrounds where the objective is to forecast and one wants to estimate how accurately a model will accomplish in practice. \n\nThe goal of cross-validation is to term a data set to test the model in the training phase (i.e. validation data set) to limit problems like overfitting and gain insight into how the model will generalize to an independent data set."},{"id":39,"text":"39. What is collaborative filtering?\nMost recommender systems use this filtering process to find patterns and information by collaborating perspectives, numerous data sources, and several agents."},{"id":40,"text":"40. Do gradient descent methods always converge to similar points?\nThey do not, because in some cases, they reach a local minima or a local optima point. You would not reach the global optima point. This is governed by the data and the starting conditions."},{"id":41,"text":"41. What is the goal of A/B Testing?\nThis is statistical hypothesis testing for randomized experiments with two variables, A and B. The objective of A/B testing is to detect any changes to a web page to maximize or increase the outcome of a strategy."},{"id":42,"text":"42. What are the drawbacks of the linear model?\nThe assumption of linearity of the errors\nIt can't be used for count outcomes or binary outcomes\nThere are overfitting problems that it can't solve"},{"id":43,"text":"43. What is the law of large numbers?\nIt is a theorem that describes the result of performing the same experiment very frequently. This theorem forms the basis of frequency-style thinking. It states that the sample mean, sample variance, and sample standard deviation converge to what they are trying to estimate."},{"id":44,"text":"44.  What are the confounding variables?\nThese are extraneous variables in a statistical model that correlates directly or inversely with both the dependent and the independent variable. The estimate fails to account for the confounding factor."},{"id":45,"text":"45. What is star schema?\nIt is a traditional database schema with a central table. Satellite tables map IDs to physical names or descriptions and can be connected to the central fact table using the ID fields; these tables are known as lookup tables and are principally useful in real-time applications, as they save a lot of memory. Sometimes, star schemas involve several layers of summarization to recover information faster."},{"id":46,"text":"46. How regularly must an algorithm be updated?\nYou will want to update an algorithm when:\n\nYou want the model to evolve as data streams through infrastructure\nThe underlying data source is changing\nThere is a case of non-stationarity"},{"id":47,"text":"47.  What are eigenvalue and eigenvector?\nEigenvalues are the directions along which a particular linear transformation acts by flipping, compressing, or stretching.\n\nEigenvectors are for understanding linear transformations. In data analysis, we usually calculate the eigenvectors for a correlation or covariance matrix."},{"id":48,"text":"48. Why is resampling done?\nResampling is done in any of these cases:\n\nEstimating the accuracy of sample statistics by using subsets of accessible data, or drawing randomly with replacement from a set of data points\nSubstituting labels on data points when performing significance tests\nValidating models by using random subsets (bootstrapping, cross-validation)"},{"id":49,"text":"49. What is selection bias?\nSelection bias, in general, is a problematic situation in which error is introduced due to a non-random population sample."},{"id":50,"text":"50. What are the types of biases that can occur during sampling?\nSelection bias\nUndercoverage bias\nSurvivorship bias"},{"id":51,"text":"51. What is survivorship bias?\nSurvivorship bias is the logical error of focusing on aspects that support surviving a process and casually overlooking those that did not because of their lack of prominence. This can lead to wrong conclusions in numerous ways."},{"id":52,"text":"52. How do you work towards a random forest?\nThe underlying principle of this technique is that several weak learners combine to provide a strong learner. The steps involved are:\n\nBuild several decision trees on bootstrapped training samples of data\nOn each tree, each time a split is considered, a random sample of mm predictors is chosen as split candidates out of all pp predictors\nRule of thumb: At each split m=p√m=p\nPredictions: At the majority rule\nThis exhaustive list is sure to strengthen your preparation for data science interview questions."},{"id":53,"text":"53. What is a bias-variance trade-off?\nBias: Due to an oversimplification of a Machine Learning Algorithm, an error occurs in our model, which is known as Bias. This can lead to an issue of underfitting and might lead to oversimplified assumptions at the model training time to make target functions easier and simpler to understand.\n\nSome of the popular machine learning algorithms which are low on the bias scale are -\n\nSupport Vector Machines (SVM), K-Nearest Neighbors (KNN), and Decision Trees.\n\nAlgorithms that are high on the bias scale -\n\nLogistic Regression and Linear Regression.\n\nVariance: Because of a complex machine learning algorithm, a model performs really badly on a test data set as the model learns even noise from the training data set. This error that occurs in the Machine Learning model is called Variance and can generate overfitting and hyper-sensitivity in Machine Learning models.\n\nWhile trying to get over bias in our model, we try to increase the complexity of the machine learning algorithm. Though it helps in reducing the bias, after a certain point, it generates an overfitting effect on the model hence resulting in hyper-sensitivity and high variance.\n\nBias-Variance trade-off: To achieve the best performance, the main target of a supervised machine learning algorithm is to have low variance and bias. \n\nThe following things are observed regarding some of the popular machine learning algorithms -\n\nThe Support Vector Machine algorithm (SVM) has high variance and low bias. In order to change the trade-off, we can increase the parameter C. The C parameter results in a decrease in the variance and an increase in bias by influencing the margin violations allowed in training datasets.\nIn contrast to the SVM, the K-Nearest Neighbors (KNN) Machine Learning algorithm has a high variance and low bias. To change the trade-off of this algorithm, we can increase the prediction influencing neighbors by increasing the K value, thus increasing the model bias."},{"id":54,"text":"54. Describe Markov chains?\nMarkov Chains defines that a state’s future probability depends only on its current state. \n\nMarkov chains belong to the Stochastic process type category.\n\nThe below diagram explains a step-by-step model of the Markov Chains whose output depends on their current state.\n\nA perfect example of the Markov Chains is the system of word recommendation. In this system, the model recognizes and recommends the next word based on the immediately previous word and not anything before that. The Markov Chains take the previous paragraphs that were similar to training data-sets and generates the recommendations for the current paragraphs accordingly based on the previous word."},{"id":55,"text":"55. Why is R used in Data Visualization?\nR is widely used in Data Visualizations for the following reasons-\n\nWe can create almost any type of graph using R.\nR has multiple libraries like lattice, ggplot2, leaflet, etc., and so many inbuilt functions as well.  \nIt is easier to customize graphics in R compared to Python.\nR is used in feature engineering and in exploratory data analysis as well."},{"id":56,"text":"56. What is the difference between a box plot and a histogram?\nThe frequency of a certain feature’s values is denoted visually by both box plots\n\nand histograms. \n\nBoxplots are more often used in comparing several datasets and compared to histograms, take less space and contain fewer details. Histograms are used to know and understand the probability distribution underlying a dataset."},{"id":57,"text":"57. What does NLP stand for?\nNLP is short for Natural Language Processing. It deals with the study of how computers learn a massive amount of textual data through programming. A few popular examples of NLP are Stemming, Sentimental Analysis, Tokenization, removal of stop words, etc."},{"id":58,"text":"58. Difference between an error and a residual error\nThe difference between a residual error and error are defined below -\n\nError\nResidual Error\nThe difference between the actual value and the predicted value is called an error.\n\n \nSome of the popular means of calculating data science errors are -\n\n \nRoot Mean Squared Error (RMSE)\n \nMean Absolute Error (MAE)\n \nMean Squared Error (MSE)\n\n \t\nThe difference between the arithmetic mean of a group of values and the observed group of values is called a residual error.\n\nAn error is generally unobservable.\n\n A residual error can be represented using a graph.\n\nA residual error is used to show how the sample population data and the observed data differ from each other.\n\n An error is how actual population data and observed data differ from each other."},{"id":59,"text":"59. Difference between Normalisation and Standardization\nStandardization\nNormalization\nThe technique of converting data in such a way that it is normally distributed and has a standard deviation of 1 and a mean of 0.\nThe technique of converting all data values to lie between 1 and 0 is known as Normalization. This is also known as min-max scaling. \nStandardization takes care that the standard normal distribution is followed by the data.\nThe data returning into the 0 to 1 range is taken care of by Normalization.\nNormalization formula -\nX’ = (X - Xmin) / (Xmax - Xmin)\n\nHere,\n\nXmin - feature’s minimum value,\n\nXmax - feature’s maximum value.\n\n \nStandardization formula -\nX’ = (X - 𝞵) / 𝞼"},{"id":60,"text":"60. Difference between Point Estimates and Confidence Interval\nConfidence Interval: A range of values likely containing the population parameter is given by the confidence interval. Further, it even tells us how likely that particular interval can contain the population parameter. The Confidence Coefficient (or Confidence level) is denoted by 1-alpha, which gives the probability or likeness. The level of significance is given by alpha. \n\nPoint Estimates: An estimate of the population parameter is given by a particular value called the point estimate. Some popular methods used to derive Population Parameters’ Point estimators are - Maximum Likelihood estimator and the Method of Moments.\n\nTo conclude, the bias and variance are inversely proportional to each other, i.e., an increase in bias results in a decrease in the variance, and an increase in variance results in a decrease in bias.\n\nOne-on-One Data Science Interview Questions\nTo crack a data science interview is no walk in the park. It requires in-depth knowledge and expertise in various topics. Furthermore, the projects that you have worked on can significantly boost your potential in a lot of interviews. In order to help you with your interviews, we have compiled a set of questions for you to relate to. Since data science is an extensive field, there are no limitations on the type of questions that can be inquired. With that being said, you can answer each of these questions depending on the projects you have worked on and the industries you have been in. Try to answer each one of these sample questions and then share your answer with us through the comments.\n\nPro Tip: No matter how basic a question may seem, always try to view it from a technical perspective and use each question to demonstrate your unique technical skills and abilities."},{"id":61,"text":"61. Which is your favorite machine learning algorithm and why?"},{"id":62,"text":"62. Which according to you is the most important skill that makes a good data scientist?"},{"id":63,"text":"63. Why do you think data science is so popular today?"},{"id":64,"text":"64. Explain the most challenging data science project that you worked on."},{"id":65,"text":"65. How do you usually prefer working on a project - individually, small team, or large team?"},{"id":66,"text":"66. Based on your experience in the industry, tell me about your top 5 predictions for the next 10 years."},{"id":67,"text":"67. What are some unique skills that you can bring to the team as a data scientist?"},{"id":68,"text":"68. Were you always in the data science field? If not, what made you change your career path and how did you upgrade your skills?"},{"id":69,"text":"69. If we give you a random data set, how will you figure out whether it suits the business needs or not?"},{"id":70,"text":"70. Given a chance, if you could pick a career other than being a data scientist, what would you choose?"},{"id":71,"text":"71. Given the constant change in the data science field, how quickly can you adapt to new technologies?"},{"id":72,"text":"72. Have you ever been in a conflict with your colleagues regarding different strategies to go about a project? How were you able to resolve it?"},{"id":73,"text":"73. Can you break down an algorithm you have used on a recent project?"},{"id":74,"text":"74. What tools did you use in your last project and why?"},{"id":75,"text":"75. Think of the last technical problem that you solved. If you had no limitations with the project’s budget, what would be the first thing you would do to solve the same problem?"},{"id":76,"text":"76. When you are assigned multiple projects at the same time, how best do you organize your time?"},{"id":77,"text":"77. Tell me about a time when your project didn’t go according to plan and what you learned from it."},{"id":78,"text":"78. Have you ever created an original algorithm? How did you go about doing that and for what purpose?"},{"id":79,"text":"79. What is your most favored strategy to clean a big data set and why?"},{"id":80,"text":"80. Do you contribute to any open source projects?"}]}