{"data":[{"id":1,"text":"1. What do you know about DevOps?\nYour answer must be simple and straightforward. Begin by explaining the growing importance of DevOps in the IT industry. Discuss how such an approach aims to synergize the efforts of the development and operations teams to accelerate the delivery of software products, with a minimal failure rate. Include how DevOps is a value-added practice, where development and operations engineers join hands throughout the product or service lifecycle, right from the design stage to the point of deployment."},{"id":2,"text":"2. How is DevOps different from agile methodology?\nDevOps is a culture that allows the development and the operations team to work together. This results in continuous development, testing, integration, deployment, and monitoring of the software throughout the lifecycle.\n\nAgile is a software development methodology that focuses on iterative, incremental, small, and rapid releases of software, along with customer feedback. It addresses gaps and conflicts between the customer and developers.\n\nDevOps addresses gaps and conflicts between the Developers and IT Operations."},{"id":3,"text":"3. Which are some of the most popular DevOps tools?\nThe most popular DevOps tools include:\n\nSelenium\nPuppet\nChef\nGit\nJenkins\nAnsible\nDocker"},{"id":4,"text":"4. What are the different phases in DevOps?\nThe various phases of the DevOps lifecycle are as follows:\n\nPlan - Initially, there should be a plan for the type of application that needs to be developed. Getting a rough picture of the development process is always a good idea.\nCode - The application is coded as per the end-user requirements. \nBuild - Build the application by integrating various codes formed in the previous steps.\nTest - This is the most crucial step of the application development. Test the application and rebuild, if necessary.\nIntegrate - Multiple codes from different programmers are integrated into one.\nDeploy - Code is deployed into a cloud environment for further usage. It is ensured that any new changes do not affect the functioning of a high traffic website.\nOperate - Operations are performed on the code if required.\nMonitor - Application performance is monitored. Changes are made to meet the end-user requirements."},{"id":5,"text":"5. Mention some of the core benefits of DevOps.\nThe core benefits of DevOps are as follows:\n\nTechnical benefits\nContinuous software delivery\nLess complex problems to manage\nEarly detection and faster correction of defects\nBusiness benefits\nFaster delivery of features\nStable operating environments\nImproved communication and collaboration between the teams"},{"id":6,"text":"6. How will you approach a project that needs to implement DevOps?\nThe following standard approaches can be used to implement DevOps in a specific project:\n\nStage 1\n\nAn assessment of the existing process and implementation for about two to three weeks to identify areas of improvement so that the team can create a road map for the implementation.\n\nStage 2\n\nCreate a proof of concept (PoC). Once it is accepted and approved, the team can start on the actual implementation and roll-out of the project plan.\n\nStage 3\n\nThe project is now ready for implementing DevOps by using version control/integration/testing/deployment/delivery and monitoring followed step by step.\n\nBy following the proper steps for version control, integration, testing, deployment, delivery, and monitoring, the project is now ready for DevOps implementation."},{"id":7,"text":"7. What is the difference between continuous delivery and continuous deployment?\nContinuous Delivery\tContinuous Deployment\nEnsures code can be safely deployed on to production\tEvery change that passes the automated tests is deployed to production automatically\nEnsures business applications and services function as expected\tMakes software development and the release process faster and more robust\nDelivers every change to a production-like environment through rigorous automated testing\tThere is no explicit approval from a developer and requires a developed culture of monitoring."},{"id":8,"text":"8. What is the role of configuration management in DevOps?\nEnables management of and changes to multiple systems.\nStandardizes resource configurations, which in turn, manage IT infrastructure.\nIt helps with the administration and management of multiple servers and maintains the integrity of the entire infrastructure."},{"id":9,"text":"9. How does continuous monitoring help you maintain the entire architecture of the system?\nContinuous monitoring in DevOps is a process of detecting, identifying, and reporting any faults or threats in the entire infrastructure of the system.\n\nEnsures that all services, applications, and resources are running on the servers properly.\nMonitors the status of servers and determines if applications are working correctly or not.\nEnables continuous audit, transaction inspection, and controlled monitoring."},{"id":10,"text":"10. What is the role of AWS in DevOps?\nAWS has the following role in DevOps:\n\nFlexible services - Provides ready-to-use, flexible services without the need to install or set up the software.\nBuilt for scale - You can manage a single instance or scale to thousands using AWS services.\nAutomation - AWS lets you automate tasks and processes, giving you more time to innovate\nSecure - Using AWS Identity and Access Management (IAM), you can set user permissions and policies.\nLarge partner ecosystem - AWS supports a large ecosystem of partners that integrate with and extend AWS services."},{"id":11,"text":"11. Name three important DevOps KPIs.\nThe three important KPIs are as follows:\n\nMeantime to failure recovery - This is the average time taken to recover from a failure.\nDeployment frequency - The frequency in which the deployment occurs. \nPercentage of failed deployments - The number of times the deployment fails."},{"id":12,"text":"12. Explain the term \"Infrastructure as Code\" (IaC) as it relates to configuration management.\nWriting code to manage configuration, deployment, and automatic provisioning.\nManaging data centers with machine-readable definition files, rather than physical hardware configuration.\nEnsuring all your servers and other infrastructure components are provisioned consistently and effortlessly. \nAdministering cloud computing environments, also known as infrastructure as a service (IaaS)."},{"id":13,"text":"13. How is IaC implemented using AWS?\nStart by talking about the age-old mechanisms of writing commands onto script files and testing them in a separate environment before deployment and how this approach is being replaced by IaC. Similar to the codes written for other services, with the help of AWS, IaC allows developers to write, test, and maintain infrastructure entities in a descriptive manner, using formats such as JSON or YAML. This enables easier development and faster deployment of infrastructure changes."},{"id":14,"text":"14. Why Has DevOps Gained Prominence over the Last Few Years?\nBefore talking about the growing popularity of DevOps, discuss the current industry scenario. Begin with some examples of how big players such as Netflix and Facebook are investing in DevOps to automate and accelerate application deployment and how this has helped them grow their business. Using Facebook as an example, you would point to Facebook’s continuous deployment and code ownership models and how these have helped it scale up but ensure the quality of experience at the same time. Hundreds of lines of code are implemented without affecting quality, stability, and security.\n\nYour next use case should be Netflix. This streaming and on-demand video company follow similar practices with fully automated processes and systems. Mention the user base of these two organizations: Facebook has 2 billion users while Netflix streams online content to more than 100 million users worldwide.\n\nThese are great examples of how DevOps can help organizations to ensure higher success rates for releases, reduce the lead time between bug fixes, streamline and continuous delivery through automation, and an overall reduction in manpower costs.\n\nWe will now look into the next set of DevOps Interview Questions that includes - Git, Selenium, Jenkins."},{"id":15,"text":"16. What are the anti-patterns of DevOps?\nPatterns are common practices that are usually followed by organizations. An anti-pattern is formed when an organization continues to blindly follow a pattern adopted by others but does not work for them. Some of the myths about DevOps include:\n\nCannot perform DevOps → Have the wrong people\nDevOps ⇒ Production Management is done by developers\nThe solution to all the organization’s problems ⇒ DevOps\nDevOps == Process \nDevOps == Agile\nCannot perform DevOps → Organization is unique\nA separate group needs to be made for DevOps"},{"id":16,"text":"17. What are the benefits of using version control?\nHere are the benefits of using Version Control:\n\nAll team members are free to work on any file at any time with the Version Control System (VCS). Later on, VCS will allow the team to integrate all of the modifications into a single version.\nThe VCS asks to provide a brief summary of what was changed every time we save a new version of the project. We also get to examine exactly what was modified in the content of the file. As a result, we will be able to see who made what changes to the project.\nInside the VCS, all the previous variants and versions are properly stored. We will be able to request any version at any moment, and we will be able to retrieve a snapshot of the entire project at our fingertips.\nA VCS that is distributed, such as Git, lets all the team members retrieve a complete history of the project. This allows developers or other stakeholders to use the local Git repositories of any of the teammates even if the main server goes down at any point in time."},{"id":17,"text":"18. Describe the branching strategies you have used.\nTo test our knowledge of the purpose of branching and our experience of branching at a past job, this question is usually asked. \n\nBelow topics can help in answering this DevOps interview question -\n\nRelease branching - We can clone the develop branch to create a Release branch once it has enough functionality for a release. This branch kicks off the next release cycle, thus no new features can be contributed beyond this point. The things that can be contributed are documentation generation, bug fixing, and other release-related tasks. The release is merged into master and given a version number once it is ready to ship. It should also be merged back into the development branch, which may have evolved since the initial release.\nFeature branching - This branching model maintains all modifications for a specific feature contained within a branch. The branch gets merged into master once the feature has been completely tested and approved by using tests that are automated.\nTask branching - In this branching model, every task is implemented in its respective branch. The task key is mentioned in the branch name. We need to simply look at the task key in the branch name to discover which code implements which task."},{"id":18,"text":"19. Can you explain the “Shift left to reduce failure” concept in DevOps?\nShift left is a DevOps idea for improving security, performance, and other factors. Let us take an example: if we look at all of the processes in DevOps, we can state that security is tested prior to the deployment step. We can add security in the development phase, which is on the left, by employing the left shift method. [will be depicted in a diagram] We can integrate with all phases, including before development and during testing, not just development. This most likely raises the security level by detecting faults at an early stage."},{"id":19,"text":"20. What is the Blue/Green Deployment Pattern?\nThis is a method of continuous deployment that is commonly used to reduce downtime. This is where traffic is transferred from one instance to another. In order to include a fresh version of code, we must replace the old code with a new code version. \n\nThe new version exists in a green environment and the old version exists in a blue environment. After making changes to the previous version, we need a new instance from the old one to execute a newer version of the instance."},{"id":20,"text":"21. What is Continuous Testing?\nContinuous Testing constitutes the running of automated tests as part of the software delivery pipeline to provide instant feedback on the business risks present in the most recent release. In order to prevent problems in step-switching in the Software delivery life-cycle and to allow Development teams to receive immediate feedback, every build is continually tested in this manner. This results in significant increase in speed in a developer's productivity as it eliminates the requirement for re-running all the tests after each update and project re-building."},{"id":21,"text":"22. What is Automation Testing?\nTest automation or manual testing Automation is the process of automating a manual procedure in order to test an application or system. Automation testing entails the use of independent testing tools that allow you to develop test scripts that can be run repeatedly without the need for human interaction."},{"id":22,"text":"23. What are the benefits of Automation Testing?\nSome of the advantages of Automation Testing are -\n\nHelps to save money and time.\nUnattended execution can be easily done.\nHuge test matrices can be easily tested.\nParallel execution is enabled.\nReduced human-generated errors, which results in improved accuracy.\nRepeated test tasks execution is supported."},{"id":23,"text":"24. How to automate Testing in the DevOps lifecycle?\nDevelopers are obliged to commit all source code changes to a shared DevOps repository.\n\nEvery time a change is made in the code, Jenkins-like Continuous Integration tools will grab it from this common repository and deploy it for Continuous Testing, which is done by tools like Selenium."},{"id":24,"text":"25. Why is Continuous Testing important for DevOps?\nAny modification to the code may be tested immediately with Continuous Testing. This prevents concerns like quality issues and release delays that might occur whenever big-bang testing is delayed until the end of the cycle. In this way, Continuous Testing allows for high-quality and more frequent releases."},{"id":25,"text":"26. What are the key elements of Continuous Testing tools?\nContinuous Testing key elements are:\n\nTest Optimization - It guarantees that tests produce reliable results and actionable information. Test Data Management, Test Optimization Management, and Test Maintenance are examples of aspects.\nAdvanced Analysis - In order to avoid problems from occurring in the first place and to achieve more within each iteration, it employs automation in areas like scope assessment/prioritization, changes effect analysis, and static code analysis.\nPolicy Analysis - It guarantees that all processes are in line with the organization's changing business needs and that all compliance requirements are met.\nRisk Assessment - Test coverage optimization, technical debt, risk mitigation duties, and quality evaluation are all covered to guarantee the build is ready to move on to the next stage.\nService Virtualization - Ensures that real-world testing scenarios are available. Service visualisation provides access to a virtual representation of the needed testing phases, ensuring its availability and reducing the time spent setting up the test environment.\nRequirements Traceability - It guarantees that no rework is necessary and real criteria are met. To determine which needs require additional validation, are in jeopardy and performing as expected, an object evaluation is used.\nDevOps Interview Questions for Source Code Management — Git"},{"id":26,"text":"27. Explain the difference between a centralized and distributed version control system (VCS).\nCentralized Version Control System\nAll file versions are stored on a central server\nNo developer has a copy of all files on a local system\nIf the central server crashes, all data from the project will be lost\nDistributed Control System\nEvery developer has a copy of all versions of the code on their systems\nEnables team members to work offline and does not rely on a single location for backups\nThere is no threat, even if the server crashes"},{"id":27,"text":"28. What is the git command that downloads any repository from GitHub to your computer?\nThe git command that downloads any repository from GitHub to your computer is git clone."},{"id":28,"text":"29. How do you push a file from your local system to the GitHub repository using Git?\nFirst, connect the local repository to your remote repository:\n\ngit remote add origin [copied web address]      \n\n// Ex: git remote add origin https://github.com/Simplilearn-github/test.git\n\nSecond, push your file to the remote repository:\n\ngit push origin master"},{"id":29,"text":"30. How is a bare repository different from the standard way of initializing a Git repository?\nUsing the standard method:\n\ngit init\nYou create a working directory with git init\nA .git subfolder is created with all the git-related revision history\nUsing the bare way\n\ngit init --bare\nIt does not contain any working or checked out a copy of source files\nBare repositories store git revision history in the root folder of your repository, instead of the .git subfolder"},{"id":30,"text":"31. Which of the following CLI commands can be used to rename files?\ngit rm\ngit mv\ngit rm -r\nNone of the above\nThe correct answer is B) git mv"},{"id":31,"text":"32. What is the process for reverting a commit that has already been pushed and made public?\nThere are two ways that you can revert a commit: \n\nRemove or fix the bad file in a new commit and push it to the remote repository. Then commit it to the remote repository using:\n\ngit commit –m \"commit message\"\n \nCreate a new commit that undoes all the changes that were made in the bad commit. Use the following command:\n\ngit revert <commit id>\nExample: git revert 56de0938f"},{"id":32,"text":"33. Explain the difference between git fetch and git pull.\nGit fetch\tGit pull\nGit fetch only downloads new data from a remote repository\tGit pull updates the current HEAD branch with the latest changes from the remote server\nDoes not integrate any new data into your working files\tDownloads new data and integrate it with the current working files\nUsers can run a Git fetch at any time to update the remote-tracking branches\tTries to merge remote changes with your local ones\nCommand - git fetch origin\n\n                  git fetch –-all\n\nCommand - git pull origin master"},{"id":33,"text":"34. What is Git stash?\nA developer working with a current branch wants to switch to another branch to work on something else, but the developer doesn't want to commit changes to your unfinished work. The solution to this issue is Git stash. Git stash takes your modified tracked files and saves them on a stack of unfinished changes that you can reapply at any time."},{"id":34,"text":"35. Explain the concept of branching in Git.\nSuppose you are working on an application, and you want to add a new feature to the app. You can create a new branch and build the new feature on that branch.\n\nBy default, you always work on the master branch\nThe circles on the branch represent various commits made on the branch\nAfter you are done with all the changes, you can merge it with the master branch"},{"id":35,"text":"36. What is the difference between Git Merge and Git Rebase?\nSuppose you are working on a new feature in a dedicated branch, and another team member updates the master branch with new commits. You can use these two functions:\n\nGit Merge\n\nTo incorporate the new commits into your feature branch, use Git merge.\n\nCreates an extra merge commit every time you need to incorporate changes\nBut, it pollutes your feature branch history\nGit Merge\n\nGit Rebase\n\nAs an alternative to merging, you can rebase the feature branch on to master.\n\nIncorporates all the new commits in the master branch\nIt creates new commits for every commit in the original branch and rewrites project history"},{"id":36,"text":"37. How do you find a list of files that have been changed in a particular commit?\nThe command to get a list of files that have been changed in a particular commit is:\n\ngit diff-tree –r {commit hash}\n\nExample: git diff-tree –r 87e673f21b\n\n-r flag instructs the command to list individual files\ncommit hash will list all the files that were changed or added in that commit"},{"id":37,"text":"38. What is a merge conflict in Git, and how can it be resolved?\nA Git merge conflict happens when you have merge branches with competing for commits, and Git needs your help to decide which changes to incorporate in the final merge.\n\nManually edit the conflicted file to select the changes that you want to keep in the final merge.\n\nResolve using GitHub conflict editor\n\nThis is done when a merge conflict is caused after competing for line changes. For example, this may occur when people make different changes to the same line of the same file on different branches in your Git repository.\n\nResolving a merge conflict using conflict editor:\nUnder your repository name, click \"Pull requests.\"\nIn the \"Pull requests\" drop-down, click the pull request with a merge conflict that you'd like to resolve\nNear the bottom of your pull request, click \"Resolve conflicts.\"\nDecide if you only want to keep your branch's changes, the other branch's changes, or make a brand new change, which may incorporate changes from both branches.\nDelete the conflict markers <<<<<<<, =======, >>>>>>> and make changes you want in the final merge.\nIf you have more than one merge conflict in your file, scroll down to the next set of conflict markers and repeat steps four and five to resolve your merge conflict.\nOnce you have resolved all the conflicts in the file, click Mark as resolved.\nIf you have more than one file with a conflict, select the next file you want to edit on the left side of the page under \"conflicting files\" and repeat steps four to seven until you've resolved all of your pull request's merge conflicts.\nOnce you've resolved your merge conflicts, click Commit merge. This merges the entire base branch into your head branch.\nTo merge your pull request, click Merge pull request.\n \nA merge conflict is resolved using the command line.\nOpen Git Bash.\nNavigate into the local Git repository that contains the merge conflict.\nGenerate a list of the files that the merge conflict affects. In this example, the file styleguide.md has a merge conflict.\nOpen any text editor, such as Sublime Text or Atom, and navigate to the file that has merge conflicts.\nTo see the beginning of the merge conflict in your file, search the file for the conflict marker \"<<<<<<<. \" Open it, and you'll see the changes from the base branch after the line \"<<<<<<< HEAD.\"\nNext, you'll see \"=======\", which divides your changes from the changes in the other branch, followed by \">>>>>>> BRANCH-NAME\".\nDecide if you only want to keep your branch's changes, the other branch's changes, or make a brand new change, which may incorporate changes from both branches.\nDelete the conflict markers \"<<<<<<<\", \"=======\", \">>>>>>>\" and make the changes you want in the final merge.\n        In this example, both the changes are incorporated into the final merge:\n\n \n\nAdd or stage your changes. \n \n\nCommit your changes with a comment.\n \n\nNow you can merge the branches on the command line, or push your changes to your remote repository on GitHub and merge your changes in a pull request."},{"id":38,"text":"39. What is Git bisect? How can you use it to determine the source of a (regression) bug?\nGit bisect is a tool that uses binary search to locate the commit that triggered a bug.\n\nGit bisect command -\n\ngit bisect <subcommand> <options>\n\nThe git bisect command is used in finding the bug performing commit in the project by using a binary search algorithm.\n\nThe bug occurring commit is called the “bad” commit, and the commit before the bug occurring one is called the “good” commit. We convey the same to the git bisect tool, and it picks a random commit between the two endpoints and prompts whether that one is the “good” or “bad” one. The process continues uptil the range is narrowed down and the exact commit that introduced the exact change is discovered."},{"id":39,"text":"40. Explain some basic Git commands.\nSome of the Basic Git Commands are summarized in the below table -\n\nCommand\nPurpose\ngit init\n\nUsed to start a new repository.\n\ngit config -\n\ngit config –global user.name “[name]”\ngit config –global user.email “[email address]”\nThis helps to set the username and email to whom the commits belong to.\n\ngit clone <repository path>\n\nUsed to create a local copy of an existing repository.\n\ngit add -\n\ngit add <file names separated by commas>\ngit add .\nUsed to add one or more files to the staging area.\n\ngit commit -\n\ngit commit -a \ngit commit -m “<add commit message>”\nCreates a snapshot or records of the file(s) that are in the staging area.\n\ngit diff -\n\ngit diff [first branch] [second branch]\ngit diff -staged\nUsed to show differences between the two mentioned branches/differences made in the files in the staging area vs current version.\n\ngit status\n\nLists out all the files that are to be committed.\n\ngit rm <file name(s)>\n\nUsed to delete a file(s) from the current working directory and also stages it.\n\ngit show <commit>\n\nShows the content changes and metadata of the mentioned commit.\n\ngit branch -\n\ngit branch [branch name]\ngit branch -d [branch name]\ngit branch\nThe first one creates a brand new branch.\n\nThe second is used to delete the mentioned branch.\n\nThe last one lists out all the branches available and also highlights the branch we are in currently.\n\nDevOps Interview Questions for Continuous Integration - Jenkins"},{"id":40,"text":"41. Explain the master-slave architecture of Jenkins.\nJenkins master pulls the code from the remote GitHub repository every time there is a code commit.\nIt distributes the workload to all the Jenkins slaves.\nOn request from the Jenkins master, the slaves carry out, builds, test, and produce test reports."},{"id":41,"text":"42. What is Jenkinsfile?\nJenkinsfile contains the definition of a Jenkins pipeline and is checked into the source control repository. It is a text file.\n\nIt allows code review and iteration on the pipeline.\nIt permits an audit trail for the pipeline.\nThere is a single source of truth for the pipeline, which can be viewed and edited."},{"id":42,"text":"43. Which of the following commands runs Jenkins from the command line?\njava –jar Jenkins.war\n\nThe correct answer is A) java –jar Jenkins.war"},{"id":43,"text":"44. What concepts are key aspects of the Jenkins pipeline?\nPipeline: User-defined model of a CD pipeline. The pipeline's code defines the entire build process, which includes building, testing and delivering an application\nNode: A machine that is part of the Jenkins environment and capable of executing a pipeline\nStep: A single task that tells Jenkins what to do at a particular point in time\nStage: Defines a conceptually distinct subset of tasks performed through the entire pipeline (build, test, deploy stages)"},{"id":44,"text":"45. Which file is used to define dependency in Maven?\nbuild.xml\npom.xml\ndependency.xml\nVersion.xml\nThe correct answer is B) pom.xml"},{"id":45,"text":"46. Explain the two types of pipeline in Jenkins, along with their syntax.\nJenkins provides two ways of developing a pipeline code: Scripted and Declarative.\n\nA. Scripted Pipeline: It is based on Groovy script as their Domain Specific Language. One or more node blocks do the core work throughout the entire pipeline.\n\nSyntax:\n\nExecutes the pipeline or any of its stages on any available agent\nDefines the build stage\nPerforms steps related to building stage\nDefines the test stage\nPerforms steps related to the test stage\nDefines the deploy stage\nPerforms steps related to the deploy stage\nB. Declarative Pipeline: It provides a simple and friendly syntax to define a pipeline. Here, the pipeline block defines the work done throughout the pipeline.\n\nSyntax:\n\nExecutes the pipeline or any of its stages on any available agent\nDefines the build stage\nPerforms steps related to building stage\nDefines the test stage\nPerforms steps related to the test stage\nDefines the deploy stage\nPerforms steps related to the deploy stage"},{"id":46,"text":"47. How do you create a backup and copy files in Jenkins?\nIn order to create a backup file, periodically back up your JENKINS_HOME directory.\n\nIn order to create a backup of Jenkins setup, copy the JENKINS_HOME directory. You can also copy a job directory to clone or replicate a job or rename the directory."},{"id":47,"text":"48. How can you copy Jenkins from one server to another?\nMove the job from one Jenkins installation to another by copying the corresponding job directory.\nCreate a copy of an existing job by making a clone of a job directory with a different name.\nRename an existing job by renaming a directory."},{"id":48,"text":"49. Name three security mechanisms Jenkins uses to authenticate users.\nJenkins uses an internal database to store user data and credentials.\nJenkins can use the Lightweight Directory Access Protocol (LDAP) server to authenticate users. \nJenkins can be configured to employ the authentication mechanism that the deployed application server uses."},{"id":49,"text":"50. How is a custom build of a core plugin deployed?\nSteps to deploy a custom build of a core plugin:\n\nCopy the .hpi file to $JENKINS_HOME/plugins\nRemove the plugin's development directory\nCreate an empty file called <plugin>.hpi.pinned\nRestart Jenkins and use your custom build of a core plugin"},{"id":50,"text":"51. How can you temporarily turn off Jenkins security if the administrative users have locked themselves out of the admin console?\nWhen security is enabled, the Config file contains an XML element named useSecurity that will be set to true.\nBy changing this setting to false, security will be disabled the next time Jenkins is restarted."},{"id":51,"text":"52. What are the ways in which a build can be scheduled/run in Jenkins?\nBy source code management commits.\nAfter completion of other builds.\nScheduled to run at a specified time.\nManual build requests."},{"id":52,"text":"53. What are the commands that you can use to restart Jenkins manually?\nTwo ways to manually restart Jenkins: \n\n(Jenkins_url)/restart            // Forces a restart without waiting for builds to complete                                \n(Jenkins_url)/safeRestart    // Allows all running builds to complete before it restarts"},{"id":53,"text":"54. Explain how you can set up a Jenkins job?\nTo create a Jenkins Job, we go to the top page of Jenkins, choose the New Job option and then select Build a free-style software project.\n\nThe elements of this freestyle job are -\n\nOptional triggers for controlling when Jenkins builds.\nOptional steps for gathering data from the build, like collecting javadoc, testing results and/or archiving artifacts.\nA build script (ant, maven, shell script, batch file, etc.) that actually does the work.\nOptional source code management system (SCM), like Subversion or CVS.\nDevOps Interview Questions for Continuous Testing - Selenium"},{"id":54,"text":"55. What are the different Selenium components?\nSelenium has the following components:\n\nSelenium Integrated Development Environment (IDE) \n\nIt has a simple framework and should be used for prototyping.\nIt has an easy-to-install Firefox plug-in.\nSelenium Remote Control (RC)\n\nTesting framework for a developer to write code in any programming language (Java, PHP, Perl, C#, etc.).\nSelenium WebDriver\n\nApplies a better approach to automate browser activities.\nIt does not rely on JavaScript.\nSelenium Grid\n\nWorks with Selenium RC and runs tests on different nodes using browsers."},{"id":55,"text":"56. What are the different exceptions in Selenium WebDriver?\nExceptions are events that occur during the execution of a program and disrupt the normal flow of a program's instructions. Selenium has the following exceptions:\n\nTimeoutException - It is thrown when a command performing an operation does not complete in the stipulated time.\nNoSuchElementException - It is thrown when an element with specific attributes is not found on the web page.\nElementNotVisibleException - It is thrown when an element is present in Document Object Model (DOM) but is not visible. Ex: Hidden Elements defined in HTML using type=“hidden”.\nSessionNotFoundException - The WebDriver is performing the action immediately after quitting the browser."},{"id":56,"text":"57. Can Selenium test an application on an Android browser?\nSelenium is capable of testing an application on an Android browser using an Android driver. You can use the Selendroid or Appium framework to test native apps or web apps in the Android browser. The following is a sample code:"},{"id":57,"text":"58. What are the different test types that Selenium supports? \nFunctional - This is a type of black-box testing in which the test cases are based on the software specification.\n\nRegression - This testing helps to find new errors, regressions, etc. in different functional and non-functional areas of code after the alteration. \n\nLoad Testing - This testing seeks to monitor the response of a device after putting a load on it. It is carried out to study the behavior of the system under certain conditions."},{"id":58,"text":"64. What are the Testing types supported by Selenium?\nThere are two types of testing that are primarily supported by Selenium:\n\nFunctional Testing - Individual testing of software functional points or features.\n\nRegression Testing - Wherever a bug is fixed, a product is retested and this is called Regression Testing."},{"id":59,"text":"65. What is Selenium IDE?\nSelenium integrated development environment (IDE)  is an all-in-one Selenium script development environment. It may be used to debug tests, alter and record and is also available as a Firefox extension. Selenium IDE comes with the whole Selenium Core that  allows us to rapidly and easily replay and record  tests in the exact environment where they will be conducted.\n\nSelenium IDE is the best environment for building Selenium tests, regardless of the style of testing we prefer, thanks to the ability to move instructions around rapidly and the autocomplete support."},{"id":60,"text":"66. What is the difference between Assert and Verify commands in Selenium?\nThe difference between Verify and Assert commands in Selenium are:\n\nThe verify commands determine whether or not the provided condition is true. The program execution does not halt regardless of whether the condition is true or not, i.e., all test steps will be completed, and verification failure will not stop the execution.\nThe assert command determines whether a condition is false or true. To know whether the supplied element is on the page or not, we do the following. The next test step will be performed by the program control, if the condition is true. However, no further tests will be run, and the execution will halt, if the condition is false."},{"id":61,"text":"67. How to launch Browser using WebDriver?\nTo launch Browser using WebDriver, following syntax is followed -\n\nWebDriver driver = new InternetExplorerDriver();\n\nWebDriver driver = new ChromeDriver();\n\nWebDriver driver = new FirefoxDriver();"},{"id":62,"text":"DevOps Interview Questions for Configuration Management — Chef, Puppet, Ansible"},{"id":63,"text":"69. Why are SSL certificates used in Chef?\nSSL certificates are used between the Chef server and the client to ensure that each node has access to the right data.\nEvery node has a private and public key pair. The public key is stored at the Chef server.\nWhen an SSL certificate is sent to the server, it will contain the private key of the node.\nThe server compares this against the public key in order to identify the node and give the node access to the required data."},{"id":64,"text":"70. Which of the following commands would you use to stop or disable the 'httpd' service when the system boots?\n# systemctl disable httpd.service\n# system disable httpd.service\n# system disable httpd\n# systemctl disable httpd.service\nThe correct answer is A) # systemctl disable httpd.service"},{"id":65,"text":"71. What is Test Kitchen in Chef?\nTest Kitchen is a command-line tool in Chef that spins up an instance and tests the cookbook on it before deploying it on the actual nodes."},{"id":66,"text":"72. How does chef-apply differ from chef-client?\nchef-apply is run on the client system.\n\nchef-apply applies the recipe mentioned in the command on the client system.\n\n$ chef-apply recipe_name.rb\n \nchef-client is also run on the client system.\n\nchef-client applies all the cookbooks in your server's run list to the client system.\n\n$ knife chef-client"},{"id":67,"text":"73. What is the command to sign the requested certificates?\nFor Puppet version 2.7:\n\n# puppetca –sign hostname-of-agent\n\nExample:\n\n# puppetca –sign ChefAgent\n\n# puppetca sign hostname-of-agent\n\nExample:\n\n# puppetca sign ChefAgent\n \nFor Puppet version 2.7:\n\n# puppetca –sign hostname-of-agent\n\nExample:\n\n# puppetca –sign ChefAgent\n\n# puppetca sign hostname-of-agent\n\nExample:\n\n# puppetca sign ChefAgent"},{"id":68,"text":"74. Which open source or community tools do you use to make Puppet more powerful?\nChanges in the configuration are tracked using Jira, and further maintenance is done through internal procedures. \nVersion control takes the support of Git and Puppet's code manager app.\nThe changes are also passed through Jenkin's continuous integration pipeline."},{"id":69,"text":"75. What are the resources in Puppet?\nResources are the basic units of any configuration management tool.\nThese are the features of a node, like their software packages or services.\nA resource declaration, written in a catalog, describes the action to be performed on or with the resource.\nWhen the catalog is executed, it sets the node to the desired state."},{"id":70,"text":"76. What is a class in Puppet?\nClasses are named blocks in your manifest that configure various functionalities of the node, such as services, files, and packages.\n\nThe classes are added to a node's catalog and are executed only when explicitly invoked.\n\nClass apache (String $version = ‘latest’) {\n\npackage{\n\n‘httpd’: ensure => $version,\n\nbefore => File[‘/etc/httpd.conf’],}"},{"id":71,"text":"77. What is an Ansible role?\nAn Ansible role is an independent block of tasks, variables, files, and templates embedded inside a playbook.\n\nThis playbook installs tomcat on node1."},{"id":72,"text":"78. When should I use '{{ }}'?\nAlways use {{}} for variables, unless you have a conditional statement, such as \"when: …\". This is because conditional statements are run through Jinja, which resolves the expressions.\n\n For example:\n\n      echo “This prints the value of {{foo}}”\n\n      when : foo is defined\n\nUsing brackets makes it simpler to distinguish between strings and undefined variables.\n\nThis also ensures that Ansible doesn't recognize the line as a dictionary declaration."},{"id":73,"text":"79. What is the best way to make content reusable/redistributable?\nThere are three ways to make content reusable or redistributable in Ansible:\n\nRoles are used to managing tasks in a playbook. They can be easily shared via Ansible Galaxy.\n\"include\" is used to add a submodule or another file to a playbook. This means a code written once can be added to multiple playbooks.\n\"import\" is an improvement of \"include,\" which ensures that a file is added only once. This is helpful when a line is run recursively."},{"id":74,"text":"80. How is Ansible different from Puppet?\nAnsible\tPuppet\nEasy agentless installation\tAgent-based installation\nBased on Python\tBased on Ruby\nConfiguration files are written in YAML\tConfiguration files are written in DSL\nNo support for Windows\tSupport for all popular OS's\nWe will now look at some DevOps interview questions on contanerization."},{"id":75,"text":"DevOps Interview Questions on Containerization"},{"id":76,"text":"81. Explain the architecture of Docker.\nDocker uses a client-server architecture.\nDocker Client is a service that runs a command. The command is translated using the REST API and is sent to the Docker Daemon (server). \nDocker Daemon accepts the request and interacts with the operating system to build Docker images and run Docker containers.\nA Docker image is a template of instructions, which is used to create containers.\nDocker container is an executable package of an application and its dependencies together.\nDocker registry is a service to host and distribute Docker images among users."},{"id":77,"text":"82. What are the advantages of Docker over virtual machines?\nCriteria\tVirtual Machine \tDocker\nMemory space\tOccupies a lot of memory space\tDocker containers occupy less space\nBoot-up time\tLong boot-up time\tShort boot-up time\nPerformance\tRunning multiple virtual machines leads to unstable performance \tContainers have a better performance, as they are hosted in a single Docker engine\nScaling\tDifficult to scale up\tEasy to scale up\nEfficiency\tLow efficiency\tHigh efficiency\nPortability\tCompatibility issues while porting across different platforms\tEasily portable across different platforms\nSpace allocation\tData volumes cannot be shared\tData volumes are shared and used again across multiple containers"},{"id":78,"text":"83. How do we share Docker containers with different nodes?\n \nIt is possible to share Docker containers on different nodes with Docker Swarm.\nDocker Swarm is a tool that allows IT administrators and developers to create and manage a cluster of swarm nodes within the Docker platform.\nA swarm consists of two types of nodes: a manager node and a worker node."},{"id":79,"text":"84. What are the commands used to create a Docker swarm?\nCreate a swarm where you want to run your manager node.\n\nDocker swarm init --advertise-addr <MANAGER-IP> \nOnce you've created a swarm on your manager node, you can add worker nodes to your swarm.\nWhen a node is initialized as a manager, it immediately creates a token. In order to create a worker node, the following command (token) should be executed on the host machine of a worker node.\n\ndocker swarm join \\ --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \\ 192.168.99.100:2377"},{"id":80,"text":"85. How do you run multiple containers using a single service?\nIt is possible to run multiple containers as a single service with Docker Compose.\nHere, each container runs in isolation but can interact with each other.\nAll Docker Compose files are YAML files."},{"id":81,"text":"86. What is a Dockerfile used for?\nA Dockerfile is used for creating Docker images using the build command.\nWith a Docker image, any user can run the code to create Docker containers.\nOnce a Docker image is built, it's uploaded in a Docker registry.\nFrom the Docker registry, users can get the Docker image and build new containers whenever they want."},{"id":82,"text":"87. Explain the differences between Docker images and Docker containers.\nDocker Images \tDocker Container\nDocker images are templates of Docker containers\tContainers are runtime instances of a Docker image\nAn image is built using a Dockerfile\tContainers are created using Docker images\nIt is stored in a Docker repository or a Docker hub\tThey are stored in the Docker daemon\nThe image layer is a read-only filesystem\tEvery container layer is a read-write filesystem"},{"id":83,"text":"88. Instead of YAML, what can you use as an alternate file for building Docker compose?\nTo build a Docker compose, a user can use a JSON file instead of YAML. In case a user wants to use a JSON file, he/she should specify the filename as given:\n\nDocker-compose -f Docker-compose.json up"},{"id":84,"text":"89. How do you create a Docker container?\n\nTask: Create a MySQL Docker container \n\nA user can either build a Docker image or pull an existing Docker image (like MySQL) from Docker Hub.\n\nNow, Docker creates a new container MySQL from the existing Docker image. Simultaneously, the container layer of the read-write filesystem is also created on top of the image layer.\n\nCommand to create a Docker container: Docker run -t –i MySQL \nCommand to list down the running containers: Docker ps"},{"id":85,"text":"90. What is the difference between a registry and a repository?\n\nRegistry\tRepository\nA Docker registry is an open-source server-side service used for hosting and distributing Docker images\tThe repository is a collection of multiple versions of Docker images \nIn a registry, a user can distinguish between Docker images with their tag names\tIt is stored in a Docker registry \nDocker also has its own default registry called Docker Hub\tIt has two types: public and private repositories"},{"id":86,"text":"91. What are the cloud platforms that support Docker?\nThe following are the cloud platforms that Docker runs on:\n\nAmazon Web Services\nMicrosoft Azure\nGoogle Cloud Platform\nRackspace"},{"id":87,"text":"92. What is the purpose of the expose and publish commands in Docker?\nExpose\nExpose is an instruction used in Dockerfile.\nIt is used to expose ports within a Docker network.\nIt is a documenting instruction used at the time of building an image and running a container.\nExpose is the command used in Docker.\nExample: Expose 8080\nPublish\nPublish is used in a Docker run command.\nIt can be used outside a Docker environment.\nIt is used to map a host port to a running container port.\n--publish or –p is the command used in Docker.\nExample: docker run –d –p 0.0.0.80:80\nNow, let's have a look at the DevOps interview questions for continuous monitoring."},{"id":88,"text":"DevOps Interview Questions for Continuous Monitoring"},{"id":89,"text":"93. How does Nagios help in the continuous monitoring of systems, applications, and services?\nNagios enables server monitoring and the ability to check if they are sufficiently utilized or if any task failures need to be addressed. \n\nVerifies the status of the servers and services\nInspects the health of your infrastructure\nChecks if applications are working correctly and web servers are reachable"},{"id":90,"text":"94. How does Nagios help in the continuous monitoring of systems, applications, and services?"},{"id":91,"text":"95. What do you mean by Nagios Remote Plugin Executor (NPRE) of Nagios?\nNagios Remote Plugin Executor (NPRE) enables you to execute Nagios plugins on Linux/Unix machines. You can monitor remote machine metrics (disk usage, CPU load, etc.)\n\nThe check_npre plugin that resides on the local monitoring machine\nThe NPRE daemon that runs on the remote Linux/Unix machine"},{"id":92,"text":"97. What are active and passive checks in Nagios?\nNagios is capable of monitoring hosts and services in two ways:\n\nActively\nActive checks are initiated as a result of the Nagios process\nActive checks are regularly scheduled\nPassively\nPassive checks are initiated and performed through external applications/processes\nPassive checks results are submitted to Nagios for processing"},{"id":93,"text":"98. What are active and passive checks in Nagios?\nActive Checks:\n\nThe check logic in the Nagios daemon initiates active checks.\nNagios will execute a plugin and pass the information on what needs to be checked.\nThe plugin will then check the operational state of the host or service, and report results back to the Nagios daemon.\nIt will process the results of the host or service check and send notifications.\nPassive Checks:\n\nIn passive checks, an external application checks the status of a host or service.\nIt writes the results of the check to the external command file.\nNagios reads the external command file and places the results of all passive checks into a queue for later processing.\nNagios may send out notifications, log alerts, etc. depending on the check result information."},{"id":94,"text":"99. Explain the main configuration file and its location in Nagios.\nThe main configuration file consists of several directives that affect how Nagios operates. The Nagios process and the CGIs read the config file.\n\nA sample main configuration file will be placed into your settings directory:\n\n/usr/local/Nagios/etc/resource.cfg"},{"id":95,"text":"100. What is the Nagios Network Analyzer?\nIt provides an in-depth look at all network traffic sources and security threats.\nIt provides a central view of your network traffic and bandwidth data.\nIt allows system admins to gather high-level information on the health of the network.\nIt enables you to be proactive in resolving outages, abnormal behavior, and threats before they affect critical business processes."},{"id":96,"text":"101. What are the benefits of HTTP and SSL certificate monitoring with Nagios?\nHTTP certificate monitoring\n\nIncreased server, services, and application availability.\nFast detection of network outages and protocol failures.\nEnables web transaction and web server performance monitoring.\nSSL certificate monitoring\n\nIncreased website availability.\nFrequent application availability.\nIt provides increased security."},{"id":97,"text":"102. Explain virtualization with Nagios.\nNagios can run on different virtualization platforms, like VMware, Microsoft Visual PC, Xen, Amazon EC2, etc.\n\nProvides the capabilities to monitor an assortment of metrics on different platforms\nEnsures quick detection of service and application failures\nHas the ability to monitor the following metrics:\nCPU Usage\nMemory\nNetworking\nVM status\nReduced administrative overhead"},{"id":98,"text":"103. Name the three variables that affect recursion and inheritance in Nagios.\nname - Template name that can be referenced in other object definitions so it can inherit the object's properties/variables.\n\nuse - Here, you specify the name of the template object that you\n\nwant to inherit properties/variables from.\n\nregister - This variable indicates whether or not the object definition\n\nshould be registered with Nagios.\n\ndefine someobjecttype{\n\n              object-specific variables ….\n\n              name template_name\n\n              use name_of_template\n\n              register [0/1]\n\n              }"},{"id":99,"text":"104. Why is Nagios said to be object-oriented?\nUsing the object configuration format, you can create object definitions that inherit properties from other object definitions. Hence, Nagios is known as object-oriented.\n\nTypes of Objects:\n\nServices\nHosts\nCommands\nTime Periods"},{"id":100,"text":"105. Explain what state stalking is in Nagios.\nState stalking is used for logging purposes in Nagios.\nWhen stalking is enabled for a particular host or service, Nagios will watch that host or service very carefully.\nIt will log any changes it sees in the output of check results.\nThis helps in the analysis of log files."}]}